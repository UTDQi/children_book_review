---
title: "Do We Judge Children's Books by Their Covers? "
subtitle: "Exploring Title, Exclamation Marks, and Reviewer Bias"
author: 
  - David Qi
thanks: "Code and data are available at: https://https://github.com/UTDQi/children_book_review"
date: today
date-format: long
abstract: "This study examines the impact of first impressions on children's book ratings, focusing on how attributes such title, page count, and review count influence ratings. Using data from Goodreads and both simple linear regression and fine-tuned transformer models, we found that book titles alone explained 7.94% of the variation in ratings, indicating the significance of first impressions. This study provide support for previous studies in concluding that children may favor the theme of excitement and celebration, which are useful in building recommendation system and selecting optimal titles."
format: pdf
number-sections: true
toc: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(arrow)
library(knitr)
library(ggplot2)


model = readRDS(file = here::here("models/model_linear.rds"))

book_data <- read_parquet(here::here("data/02-analysis_data/cleaned_books.parquet"))


book_data =book_data %>% filter( ratings_count > 0)

# Number of days since beginning of the year
book_data =book_data %>% mutate( publication_day_in_year =   as.integer(format(as.Date(paste(book_data$publication_year, book_data$publication_month, book_data$publication_day, sep = "-")), "%j")),
                                 text_reviews_ratio = text_reviews_count/ratings_count
                                 )
```


# Introduction {#sec-intro}
The rise of user-generated content has transformed how consumers interact with media, especially when it come to book ratings. These ratings serve as an indirect reflection of a book's quality and readers' perceptions, serving as one of the few pieces of information that help potential readers to form initial impressions. As they play a crucial role in guiding decisions, consequently, it has become standard practice to use book ratings as a predictor variable in recommendation systems, serving as a key indicator of how much a reader enjoys a book. 

However, children's books, as a unique genre, present distinct dynamics in the realm of user-generated ratings and recommendation systems. Unlike adult literature, children rarely contribute directly to ratings or reviews. Instead, parents, educators, and caregivers typically provide feedback. Which have made is difficult to create recommendation systems [@judgecover]. And since many reviewer are not direct reader of the book, we hypothesize their rating may subject to greater bias from first impression. 

This paper aims to use simple linear regression models and fine-tuned pretrained transformer models, by analyzing children's book data from Goodreads[@goodreaddata1;@goodreaddata2], to uncover how much such first impression will influence the rating of a book. The study provides guidance for author and publishers to create attractive titles, identifies traits in children's book rating, which may be useful for designing recommendation systems. Where these recommendation systems, when it comes to children, may be difficult to create due to lack of user generated data, and will likely benefit from understanding provided by this study.

As a result, we found that one may explain 7.94% variation in the data by title alone. This fact along with the significant F statistics of linear regression model, have reflected the small but clearly present influence of first impression on book ratings. We have develop theories, by looking at our model output and relating to relevant literature, that as a sign of excitement and celebration, exclamation marks can provide a positive first impression and improve rating. We also note the influence of number of pages and review to rating ratio on average rating, and considered possible causation. 

The R scripts in this paper are written in R[@citeR], The table in our study are generated by knitr [@knitr],  and graph with ggplot2[@ggplot2]. Data input and output were done using arrow[@arrow].Data manipulation made use of tidyverse[@tidyverse]. The data download, cleaning and transformer model made use of python[@citePython] with supporting packages pytorch[@NEURIPS2019_9015], Scikit-learn[@pedregosa2011scikit], transformers[@wolf-etal-2020-transformers], tqdm[@tqdm], numpy[@harris2020array], pandas[@mckinney2010data], requests[@requests].

The rest of the paper is structured as follows: in [Data](#sec-data) section we will discuss the various aspects of the data we use, how they were gathered and the variable we shall use. In the [Model](#sec-model) section we will discuss details of our models, why it is justified and the coefficient summaries.  In the [Results](#sec-results) section, we present the predictions of the model. In the [Discussion](#sec-discussion) section we will discuss the interpretation to the predictions, and what we may learn from the coefficients for the model, we will also look as possible weakness and future improvement for the model, data collection, and analysis. 

# Data {#sec-data}
We will be using the children's book data from goodreads.com, collected in late 2017, originally for the propose of academic research conducted by @goodreaddata1 and @goodreaddata2. We would also thank @tellingstories's book for providing information on this dataset.

## Overview
The children's book dataset contains 124,082 books. After removing books that lacks the requires information, or that have incorrect entries easily identifiable(such as having June 31 as date), we have selected 65,118 books for analysis. The justification and risks of such selection will be discussed in [Discussion](#).

## Data Measurement
Goodreads is an American book recommendation and social networking platform. Its primary purpose is to allow users to catalog their books, keep track of their reading progress, share reviews, and engage in discussions about literature. The included books are mainly in English, with some books in other languages available. Any registered user on Goodreads may give rating to any book, the rating comes in integers from 1 to 5, with no half-star option. Along with a rating, user also may write text reviews for the book, which may be seen by other users. 

The data collection was done through a web-scarping on contents available for any user[@goodreaddata1;@goodreaddata2]. 

## Varibles
For this study, we intend to use average rating as an estimand, and select predictor variables that has influence one one's first impression. Below is a brief overview of these variables.

### Average Rating
[@fig-ratingoverview] displays the distribution of average book ratings('average_rating'), which appears to approximate a normal distribution centered around 3.88. The data suggests a tendency for people to give higher ratings. An average book will not receive a 3, but will have average rating close to 3.88 instead. 

```{r}
#| label: fig-ratingoverview
#| fig-cap: Histogram on average rating
#| include: true
#| echo: false
#| warning: false
#| message: false

ggplot(book_data, aes(x = as.numeric(average_rating))) + 
  geom_histogram(binwidth = 0.1, fill = "skyblue", color = "black", alpha = 0.7) +
  labs(
    title = "Histogram of Average Ratings",
    x = "Average Rating",
    y = "Count(Books)"
  ) +
  theme_minimal()
```

### Publication Day
Publication day(`publication_day`) is the day in the month the book is published, it is used to capture monthly trends. Such pattern do exists, as [@fig-pub_day_overview] clearly shows that there is a great increase in book published one the first day of each month, and a slight rise the fifteenth day. We would expect the difference in level of competition between different days in a month can influence people's perception toward a book.  

```{r}
#| label: fig-pub_day_overview
#| fig-cap: Distribution of Publication Days
#| include: true
#| echo: false
#| warning: false
#| message: false

# Convert publication_day to numeric (if it's not already) and plot
ggplot(book_data, aes(x = as.numeric(publication_day))) + 
  geom_bar(fill = "lightgreen", color = "black", alpha = 0.7) +
  labs(
    x = "Publication Day",
    y = "Count(Books)"
  ) +
  theme_minimal()
```

### Publication Day In The Year
Publication Day In The Year(`publication_day_in_year`) is the day in the year the book is published, it is used to capture yearly trends. Similar to previous part, [@fig-pub_day_yearly_overview] is a bar chart that shows the yearly pattern on publication of books. We Observe a decline in book sales in June and December, and peaks in January and Sepetember. 

```{r}
#| label: fig-pub_day_yearly_overview
#| fig-cap: Distribution of Publication Days(Yearly)
#| include: true
#| echo: false
#| warning: false
#| message: false

# Convert publication_day to numeric (if it's not already) and plot
ggplot(book_data, aes(x = as.numeric(publication_day_in_year))) + 
  geom_bar(fill = "lightgreen", color = "black", alpha = 0.7) +
  labs(
    x = "Publication Day",
    y = "Count(Books)"
  ) +
  theme_minimal()
```

### Publication Year
Publication Year is the year the book is published, people may treat old books differently from new books in the way they give reviews. Also as [@fig-pub_year_overview] shows, there is an exponential increase in the number of books people can choose form. The drop in the end of the graph is because that many newly published books need time for then to gain their first rating, and the book published in 2018 haven't been published at the time of data collection and naturally don't have reviews. 
```{r}
#| label: fig-pub_year_overview
#| fig-cap: Distribution of Publication Year
#| include: true
#| echo: false
#| warning: false
#| message: false

# Convert publication_day to numeric (if it's not already) and plot
ggplot(book_data, aes(x = as.numeric(publication_year))) + 
  geom_bar(fill = "lightgreen", color = "black", alpha = 0.7) +
  labs(
    x = "Publication Year",
    y = "Count(Books)"
  ) +
  theme_minimal()
```


### Number of Text Reviews
Number of text reviews (`text_reviews_count`) is the number of written text reviews for a book. The number of text reviews is visible to new readers, and provides them with a first impression for the popularity of the book. [@fig-textreview] shows the distribution of books with different number of text reviews, note that this is not a full plot as there are very few books that received far more than 1000 reviews, the most being 31,536 reviews. We see that of most book getting very few review and a few book have very high reviews. 

```{r}
#| label: fig-textreview
#| fig-cap: Histogram on Number of Text Review
#| include: true
#| echo: false
#| warning: false
#| message: false

ggplot(book_data, aes(x = as.numeric(text_reviews_count))) + 
  geom_histogram(binwidth = 0.1, fill = "skyblue", color = "black", alpha = 0.3) +
  labs(
    title = "Histogram of Number of Text Review",
    x = "Number of Text Review",
    y = "Books(Log Scale)"
  ) + 
  xlim(0, 1000) +  # Set x-axis limit 
  scale_y_log10()+  # Set y-axis to log scale
  theme_minimal()
```

### Text Reviews To Rating Ratio
Text Reviews To Rating Ratio (`text_reviews_ratio`) is the ratio between number of text review and number of ratings. It serves as a measurement of reader engagement and the atmosphere of the reader community. Which can have substantial impact on one's impression of a book.  [@fig-textreviewratio] shows the distribution of books with different number of this ratio, showing readers have different level of engagement for different books. Note this variable displays a weak correlation(correlation = 0.2112446) with year of publication, including this variable in the model will minimize the effect of year of publication in the linear regression model. However we have included both variables in consideration for interpretiblity. 

```{r}
#| label: fig-textreviewratio
#| fig-cap: Histogram on Ratio of Text Review
#| include: true
#| echo: false
#| warning: false
#| message: false

ggplot(book_data, aes(x = as.numeric(text_reviews_ratio))) + 
  geom_histogram(binwidth = 0.05, fill = "skyblue", color = "black", alpha = 0.7) +
  labs(
    x = "Text Review/Ratings Ratio",
    y = "Count(Books)"
  ) +
  theme_minimal()
```
### Title
Title (`title`) is the most visible trait on the front cover of a book. The wording of the title will directly shape one's first impression of the book, and directly determines if one have interest in the book. 

### Number of pages
Number of Pages (`num_pages`) is the sometimes more notable than the title of the book, one often realize how thick a book is without knowing the title. If we want to evaluate the impact of first impression on book reviews, number of pages is an important factor to take into consideration. [@fig-num_pages_overview] shows that most books in the dataset have a low number of pages, as the number of pages increases, the frequency of books with higher page counts drops off significantly. Agreeing with our impression of children;s book.

```{r}
#| label: fig-num_pages_overview
#| fig-cap: Histogram on Ratio of Text Review
#| include: true
#| echo: false
#| warning: false
#| message: false


ggplot(book_data, aes(x = as.numeric(num_pages))) + 
  geom_histogram(binwidth = 20, fill = "skyblue", color = "black", alpha = 0.7) +
  labs(
    x = "Number of pages",
    y = "Count(Books)"
  ) + xlim(0,1000)+
  theme_minimal()

```
## Unused Varibles
There are certain variables that could be drawn from the dataset, which may fit the topic of this paper, but will not be used for modeling. In this section we justify why they are not selected. 

### Publication Month
The effect of publication Month have been fully covered by days in a year. While the latter is more accurate. 

### Rating Count
The total number of ratings is an important indicator of book popularity. However including it does not improve the model by much, and it can be captured by text review to rating ratio, along with number of text reviews. So we did not inculde the varible for simplicity. 

# Model {#sec-model}

## Linear Regression Model 
We will first attempt to use linear regression model to predict the average rating of a book. 

### Model set-up
For each book, we assumed they are being sampled similarly. \
Let the rating of a book be denoted by $R$. 
Then we will use linear regression model: 
$$
R = \beta_0 + \beta_1P_{day} +  \beta_2P_{day\_in\_year} +\beta_3Pages+\beta_4Reviews+\beta_5Review\_ratio+\beta_6P_{year}+ \epsilon
$$
Where $\epsilon$ is an independent, normally distributed error term.  \
$\beta_0,\beta_1,\beta_2,\beta_3,\beta_4,\beta_5,\beta_6$ are regression coefficients. \
$P_{day}$ is the day in the month. \
$P_{day\_in\_year}$ is the number of day in the year. \
$Pages$ is the number of pages. \
$Reviews$ is the number of text reviews.\
$Review\_ratio$ is the ratio of text reviews.\
$P_{year}$ is the year of publication.\
We will predict the average rating to be: \
$$
\hat{R} = \beta_0 + \beta_1P_{day} +  \beta_2P_{day\_in\_year} +\beta_3Pages+\beta_4Reviews+\beta_5Review\_ratio+\beta_6P_{year}
$$
We fit this model using standard linear regression: finding the set of regression coefficients that minimized mean squared error. 
The model diagnostics are in [Appendix 2](#appendix2)



## Transformer Model
To analysis how title may affect the rating of a book, we use a transformer model to process the title. To implement this model, we used python[@citePython] with supporting packages pytorch[@NEURIPS2019_9015], Scikit-learn[@pedregosa2011scikit], transformers[@wolf-etal-2020-transformers], tqdm[@tqdm], numpy[@harris2020array], and pandas[@mckinney2010data].

Bidirectional Encoder Representations from Transformers (BERT) [@devlin2019bert] is a language model built on the Transformer encoder architecture, which has been pretrained on text sequence from Wikipedia and BookCorpus [@zhu2015aligning].

We will fine tune the BERT model to predict average rating from the book title. A book title is a string input, which first passes through BERT pretrained tokenizer to be made into a sequence of tokens. For example the title "Bradford Street Buddies : Springtime Blossoms" will become:\
$$
\text{
[CLS], bradford, street, buddies, : , spring, -time, blossoms, [SEP], [PAD], [PAD]
}\
$$
Where [CLS] is special token for "class", [SEP] is special token for “separator”, and [PAD] is special token for “padding”. The pretrained BERT model accept this tokenized input and outputs a token-wise embeddings tensor that encodes meaning and information related to each token as it learned from pretrained data. As BERT have been pretrained on massive data, the embedding it produces has real connections with Semantic and Syntactic meaning of the token. While the embedding related to the [CLS] token carries information regarding the overall meaning of the title[@devlin2019bert;@diveintodeeplearning]. 

For our specific use, we need to fine-tune the model. Which is preformed by adding a fully connected layer after the output related to [CLS], then train the resulting model with this new layer added, @fig-bert from original paper of @devlin2019bert  illustrated this process when the task is classification. In this case we are replacing the classification fully connected layer by a regression layer. Fine tuning BERT for regression tasks is also a common practice, with examples demonstrated by people such as @diveintodeeplearning and @pharmaceutics14081710. 


![BERT fine tuning process](bert.png){#fig-bert}

We use a 70%,15%,15% train,validation,test separation. We train the model on training data with mean squared error loss on validation set to measure model performance. We then adjust the learning rate, number of epoch trained, and batch size until we find a optimal hyperparameter that produces the best model on validation data. 
We then choose this model for further discussion. 


 
# Results {#sec-results}

## Linear Regression Model

[@tbl-model] are the model coefficients fitted. The model have adjusted R-squared 0.038, showing the model have limited predictive power. However, the F-statistic(426 on 6 and 65111 Degrees of Freedom) and Pr(>|t|) provide concrete evidence that these first impression variables that we draw out have influence on average rating. Discussion of each variable is in the [Discussion ](@sec-discussion) section. 

```{r}
#| label: tbl-model
#| tbl-cap: Linear Regression Model summary
#| include: true
#| echo: false
#| warning: false
#| message: false


model_summary <- summary(model)


coefficients <- as.data.frame(model_summary$coefficients)



coefficients <- coefficients %>%
  mutate_all(~ signif(. , 3))

# Disable scientific notation for 'Estimate' and 'Std. Error' columns
coefficients$Estimate <- format(coefficients$Estimate, scientific = FALSE)
coefficients$`Std. Error` <- format(coefficients$`Std. Error`, scientific = FALSE)


# Rename the predictor variables
rownames(coefficients) <- c("(Intercept)", "Publication Day", "Publication Year", "Number of Pages", "Number of Text Reviews", "Text Review Ratio", "Publication Day in The Year")
colnames(coefficients) = c("	Estimate", "Standard Error", "T value","P value")

kable(coefficients)
```
### Note on Publication Year
This variable's effect is not statistically significant, which requires justification for its inclusion in the model. The rationale lies in its role as an underlying factor influencing the Text Reviews Ratio, a key variable associated with higher ratings. There is a clear trend: as the Publication Year approaches the present, the Text Reviews Ratio tends to increase on average, indicating higher levels of reader engagement. This suggests that more recent publications garner greater textual interaction, which, in turn, may positively impact overall ratings.



## BERT regression Model prediction
After some testing, we have decided on a final model.[@tbl-BERT] shows the final hyperparemeter settings. The model have resulted a mean squared error loss of 0.1271 and R squared of 0.0794 on test dataset, with mean squared error of 0.1219 on validation dataset. This means that the model explains 7.94% of the variation in average ratings based solely on the book title, suggesting the influence of first impressions on how books are rated.

We have also tested with modified test data, by changing periods into exclamation marks, or removing every "the" from the title. The model suggests that by replacing every period with exclamation marks, the average rating of the test set would increase by 0.026, and by removing every "the", the rating drops by 0.012. 

```{r}
#| label: tbl-BERT
#| tbl-cap: hyperparameter setting
#| include: true
#| echo: false
#| warning: false
#| message: false

# Create a data frame with the hyperparameter settings
hyperparameters <- data.frame(
  Hyperparameter = c("MAX_LEN", "BATCH_SIZE", "EPOCHS", "LEARNING_RATE"),
  Value = c(30, 16, 2, 0.000013)
)

hyperparameters$Value[4] <- format(hyperparameters$Value[4], scientific = FALSE)

# Display the table using kable from knitr
kable(hyperparameters)
```



# Discussion{#sec-discussion}

## Interpretation of Model

### Linear Regression
The linear regression model constructed using numerical data have provide many interesting findings around each variables. And have pointed out directions for future research: using experiment and surveys to verify the causation we have proposed. However there are many limitations to linear regression model, the underlying assumptions need to be fulfilled, it have difficulties processing text data such as titles, which is why we proposed using BERT model for a more thorough analysis of this data. 

Below we discuss the result for some particular interesting variables.

#### Publication Day 
Although the change per day is small, it is highly significant. This suggests that books published later in the calendar year (closer to December) might receive slightly higher ratings on average. The relationship is positive, but the effect size is quite small. This possibly reflect the high competition on the first day of each month have damaged rating for book published on that day.

#### Number of Pages
For each additional page in a book, the average rating increases by 0.000616.
Books with more pages tend to have slightly higher average ratings. This suggests that readers might associate longer books with higher quality, that they may tend to believe the book is more in depth, more rigorous and is of higher value. We may also link this to the psychological phenomenon of cognitive dissonance, where some people may tend persuade themselves to believe the book is good to revolve the discomfort of purchasing something that is worthless [@festinger1959cognitive].

#### Text Reviews Number
There is a positive relationship between the number of text reviews and the average rating, meaning that books with more reviews are likely to have higher ratings. Which may be explained by that social proof or engagement through reviews influences ratings positively.

#### Text Reviews Ratio
For each 1% increase in the ratio of text reviews to total reviews, the average rating increases by 0.0763.
This is one of the most influential variables in the model. Books that have a higher ratio of text reviews and more detailed feedback from readers tend to receive significantly higher ratings. Indicating that detailed text-based reviews are more strongly associated with higher ratings than numerical ratings. We could also explain association by the positive impression created through higher engagement. 

#### Publication Day in the Year
Like the publication day variable, the publication day in the year also shows a positive relationship with the average rating. This means that books published later in the calendar year may be slightly more favorably rated, although the effect is again small. 

### BERT Model
The fine tuned BERT model have provided understanding on our starting question: how much do people judge a book by its cover? Our answer is that by title alone, we can explain 7.94% of the variance. While the model can be used to provide guidance in creating individual titles. It may also be experimented to see the overall effect of having one type of title over another. 

For example, our experiment of replacing periods with exclamation marks have aligned with the result of @judgecover, who find that "young children are inclined towards joy, favoring books including terms like celebration, excitement, gratitude, cheerful, and smile". Which are hinted by the use of exclamation marks. This has reinforced our previous understanding of the subject, and further verifies the predictions of our model. 
While the lowered rating by removing "the" may come from the resulting grammatical error in some books. There are other similar experiments one can do with this model to further develop knowledge on how wording of the title affect reader perception. One example would be replacing masculine pronouns into feminine or neutral pronouns, and observe the effect it has on average rating. And alternative testing preformed by removing the exclamation marks and replacing them with period could be conducted.

## Weaknesses and Next Steps
There may be several important limitations in our study and data which restrict the effectiveness of our result, we will discuss three of them below and also provide possible direction for future studies to address these limitations.


### Correlation Versus Causation
In the paper we have observed various Correlations, however, they may not demonstrate a causation. For example, we attempts to explain variation in score by text reviews ratio, however there is a important potential confounder variable, which we are not investigating since the nature of research is to find relation of first impression to ratings. This confounder variable is the quality of the book, a high quality book will naturally receive positive reviews, and may potentially attract people to comment more on the book. Where in the case of number of pages, a very important variable that we do not have hold of is formatting, larger fonts may improve reader experience while resulting in pages. 
A more comprehensive study that, likely in form of a survey may be applied to uncover causation, and whether our explanation are accurate. 

### Missing Data
There are certain important facts about the book that highly link to first impression that we can't acquire form this dataset.Including fonts, page size and paper quality. 

Also around half of of the data point are removed due to missing or incorrect data. As there may be systematic trends to what data are missing(e.g. data for popular books are less likely to be wrong since more people concerns about it), removing these data may have systematically introduced bias into our data. 

### Sources of Bias.
There may be other sources of data not from the data cleaning process. An an American website, Goodread is not representative for books in other languages than English. Not only are the books of these languages, the readers from other cultures may not be well represented. Which increase this bias further. 
Also there may be economical and technological source of bias. Books that are more accessible or affordable to readers in the U.S.  may dominate the data. Titles from smaller publishers or independent authors, particularly those outside of major Western markets, may not receive the same level of visibility or reviews.
While since the data is form an online platform, it only allow data collection on people who have the money and skill to access such platforms. Which will exclude many of the young children and elderly. As well as views from people in poverty. 
Further research could be targeted specifically toward these groups. In fact, as @judgecover points out, there are many existing, "state of the art" recommendation algorithm designed for adults. However, similar algorithms experience difficulties to be implemented on children, as there are not similar amount of data available. This lack of data have increased the importance further research through traditional methods such as surveys. See [Appendix 1](@appendix1) for a discussion on how such survey could be conducted.






\newpage

\appendix



\newpage

# Appendix 1: Ideal methodology and survey {#appendix1}

In the [discussion](@sec-discussion) section, we touched on using surveys to validate and further develop our existing findings. In this section, we present description to an idealized survey such that one may use for further research into the topic. 

## Objective
We want to validate and measure the actual effects of three of our main findings:

1. Books expressing themes such as celebration, excitement, gratitude, cheerful, and smile are preferred. While exclamation in the title hints this. 

2. Higher number of pages create positive attitude toward the book.

3. High level of community engagement create positive attitude toward the book.

## Sampling Approach
Keep in mind that this survey aims to be targets to collect more data one a particularly hard-to-reach group: children. While we also aims at identifying the part of rating pattern in children that are due to adults. Quota sampling is quite suitable for this circumstance, which enables us to place more effort on the groups we want to focus on[@chen2018methods].

## Stratified Quotas
We will use the age groups used by @judgecover to test the alignment of results, which are under 5, 6 to 8, and 9 to 11.The quota should be evenly spread geographically in the United States, and equal in age group. For a total of 500 participants, we will recruit 100 participants from each age group, evenly spread geographically. And the rest 100 is left to parents and teachers. The sample will be offline, since we do not expect to reach our target population online. 

## Incentives
We will offer the following incentives to complete the survey:

1. A letter explaining the propose, theme and length and possible questions of the survey. 

2. A $20 reward for completing the survey questionnaire. 

3. Selection of any book or book series with market price less than 30$. Which will be mailed within 30 days of completing the survey questionnaire. 

These incentives matches the outline given by @stantcheva2023surveys. And may incentivize the participants to provide more accurate replies. 

## Survey Structure 
The survey will begin with an introduction explaining its purpose, scope, and confidentiality terms. The participant may continue if they and their guardian fully understands and agrees to the terms and conditions. 

The survey questions will be grouped logically, for children it will covering the topics below:

- Demographics: Collecting background information on participant.

- Reading practices: Amount of reading done by participant lately, which genres and theme is the participant in favor of. Question regarding book length will be in this section. 

- Understanding of Rating: Should book rating reflect experiences apart from reading, such as community engagement?

- Rating of Sample Titles: Several random titles are provided for the participants to rate their first impression. Questions will randomly include real book titles, and book titles with period replace by exclamation marks.

For adults, the "reading practices" section will be replaces by a section collecting the participant's experience with children's book. And the Understanding of Rating will be from a adult perspective, asking questions such as "Will you write reviews based on your chlidren's fondness of the book?"


## Ethical Considerations
As we are interviewing children, ensuring the ethical integrity of the survey is crucial.The following considerations are taken into account:

1. **Informed Consent**:  
   - Both parents/guardians and children must provide informed consent before participating.
   - Consent forms will clearly explain the purpose, procedures, risks, benefits, and data confidentiality.
   
2. **Confidentiality and Data Protection**:  
   - Personal information collected will be anonymized and securely stored.
   - Data will be used solely for research purposes and will not be shared with third parties.

3. **Minimizing Harm and Stress**:  
   - Survey questions will be age-appropriate, avoiding sensitive or distressing topics.
   - Guardians will be encouraged to assist younger participants if needed to reduce any potential stress.

4. **Review by an Ethics Board**:  
   - The survey design will be submitted to an Institutional Review Board (IRB) or equivalent ethics committee for approval to ensure adherence to ethical standards.

5. **Child-Centric Language and Format**:  
   - The survey will be designed using simple, clear language suitable for children.
   - Visual aids, such as illustrations or emojis, may be used to engage younger participants and enhance understanding.

By addressing these ethical considerations, the survey ensures the protection and well-being of participants while maintaining the integrity and reliability of the research findings.

A sample survey can be found at: [Link](https://docs.google.com/forms/d/e/1FAIpQLSeIMsn9aL4JJsZOZtFYFiE_HoZZbaU_g5v84bLXZpoZKjtNpQ/viewform?usp=sf_link).Thank @stantcheva2023surveys and @chen2018methods's paper in providing valuable guidance in creating survey.


# Appendix 2: Model diagnostics {#appendix2}

[@fig-diagnostics] are the model diagnostics for the model. Q-Q plot is generally acceptable. There are certain influential points in the data set, however, these points does not occur incorrectly and result of some very popular books, so we are not removing them from the data set. There are some potential violation of linear regression assumptions visible from the residue vs. fitted plot, however we will accept this model for now since we have better model available and this only serves as a help in interpreting the numerical varibles. 


```{r}
#| label: fig-diagnostics
#| fig-cap: Model diagnostics for Linear regression model
#| include: true
#| echo: false
#| warning: false
#| message: false
#| fig.width: 10
#| fig.height: 7

par(mfrow = c(2, 2))  
plot(model)
```


## Selection

Our model selection is based on AIC(Akaike information criterion) and interpretation of data.

The best model in terms of AIC in selecting was "average_rating  ~  publication_day + publication_month + num_pages + text_reviews_count  +  text_reviews_ratio + ratings_count  + publication_day_in_year", the reason for selecting 'publication_year' and un-selecting 'publication_month' and 'ratings_count' have been discussed in the paper. 


\newpage

# References



