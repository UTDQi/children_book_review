{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This file Contains code to download the goodread chlidrens book data."
      ],
      "metadata": {
        "id": "iJqGcE5EifsE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlqTxE3ebkja"
      },
      "source": [
        "# Download datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2-rSgRybkjc"
      },
      "source": [
        "### Download the goodread data set. Most codes are directly from sample code provided by the data collector, see [their page](https://https://github.com/MengtingWan/goodreads) and the paper body for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1UUl519Pbkjd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "from google.colab import files\n",
        "import gzip\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tgrlpZDebkjh"
      },
      "outputs": [],
      "source": [
        "def download(local_filename):\n",
        "      url = \"https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_books_children.json.gz\"\n",
        "      with requests.get(url, stream=True) as r:\n",
        "          r.raise_for_status()\n",
        "          with open(local_filename, 'wb') as f:\n",
        "              for chunk in r.iter_content(chunk_size=8192):\n",
        "                  f.write(chunk)\n",
        "      print('Dataset', \"goodreads_books_children.json.gz\", 'has been downloaded!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhXcMc_9bkji",
        "outputId": "d0937a29-7740-4b29-8a7e-3991da80495e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset goodreads_books_children.json.gz has been downloaded!\n"
          ]
        }
      ],
      "source": [
        "OUT_DIR = './genre'\n",
        "if not os.path.exists(OUT_DIR):\n",
        "    os.makedirs(OUT_DIR)\n",
        "\n",
        "output_path = os.path.join(OUT_DIR, 'goodreads_books_children.json.gz')\n",
        "download(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "def load_data(file_name, head = 500):\n",
        "    count = 0\n",
        "    data = []\n",
        "    with gzip.open(file_name) as fin:\n",
        "        for l in fin:\n",
        "            d = json.loads(l)\n",
        "            count += 1\n",
        "            data.append(d)\n",
        "\n",
        "            # break if reaches the 100th line\n",
        "            if (head is not None) and (count > head):\n",
        "                break\n",
        "    return data"
      ],
      "metadata": {
        "id": "IVsG-6jXFrQO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to flatten nested data\n",
        "def flatten_book_data(book):\n",
        "    flat_data = {\n",
        "        \"isbn\": book.get(\"isbn\", \"\"),\n",
        "        \"title\": book.get(\"title\", \"\"),\n",
        "        \"average_rating\": book.get(\"average_rating\", \"\"),\n",
        "        \"format\": book.get(\"format\", \"\"),\n",
        "        \"publisher\": book.get(\"publisher\", \"\"),\n",
        "        \"publication_year\": book.get(\"publication_year\", \"\"),\n",
        "        \"num_pages\": book.get(\"num_pages\", \"\"),\n",
        "        \"ratings_count\": book.get(\"ratings_count\", \"\"),\n",
        "        \"text_reviews_count\": book.get(\"text_reviews_count\", \"\"),\n",
        "        \"link\": book.get(\"link\", \"\"),\n",
        "        \"authors\": \"; \".join([f'{a[\"author_id\"]} ({a[\"role\"]})' for a in book.get(\"authors\", [])]),\n",
        "        \"popular_shelves\": \"; \".join([shelf[\"name\"] for shelf in book.get(\"popular_shelves\", [])]),\n",
        "    }\n",
        "    return flat_data"
      ],
      "metadata": {
        "id": "i18kRvK_e-W4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_data = load_data(output_path,head = None)\n",
        "\n",
        "# Flatten the data\n",
        "if isinstance(book_data, list):\n",
        "    flattened_data = [flatten_book_data(book) for book in book_data]\n",
        "else:\n",
        "    flattened_data = flatten_book_data(book_data)"
      ],
      "metadata": {
        "id": "lJ6DbOFUJTkt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save this data to local device\n",
        "Code below download the data to this device."
      ],
      "metadata": {
        "id": "y1fjiBiQZ0AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save raw data\n",
        "def process_in_chunks(book_data, chunk_size=1000, output_file=\"./genre/raw_books_chunk.parquet\"):\n",
        "    \"\"\"Process the data in chunks to reduce memory consumption and append to Parquet.\"\"\"\n",
        "    chunk = []\n",
        "\n",
        "    # Start by writing the first chunk to the Parquet file\n",
        "    for i, book in enumerate(book_data):\n",
        "        # Flatten the book data\n",
        "        flattened_data = flatten_book_data(book)\n",
        "        chunk.append(flattened_data)\n",
        "\n",
        "        # When chunk size is reached or it's the last chunk, process and clear memory\n",
        "        if (i + 1) % chunk_size == 0 or i + 1 == len(book_data):\n",
        "            # Convert to DataFrame\n",
        "            df_chunk = pd.DataFrame(chunk)\n",
        "\n",
        "            # Check if the file already exists\n",
        "            try:\n",
        "                # Try reading the existing Parquet file to append\n",
        "                existing_df = pd.read_parquet(output_file)\n",
        "                # Concatenate the new chunk to the existing data\n",
        "                df_chunk = pd.concat([existing_df, df_chunk], ignore_index=True)\n",
        "            except FileNotFoundError:\n",
        "                # If the file doesn't exist, this is the first chunk to write\n",
        "                pass\n",
        "\n",
        "            # Write the concatenated DataFrame to Parquet (overwrites the file if it exists)\n",
        "            df_chunk.to_parquet(output_file, index=False)\n",
        "\n",
        "            # Clear the chunk list to free memory\n",
        "            chunk = []\n",
        "            print(f\"Processed {i + 1} books.\")\n",
        "\n",
        "raw_parquet_file = \"./genre/raw_books_chunk.parquet\"\n",
        "process_in_chunks(book_data, output_file=raw_parquet_file)\n",
        "files.download(raw_parquet_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ILrdcUs_Frol",
        "outputId": "41129dc7-d0f7-437d-c62f-58e6b5c4fcf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1000 books.\n",
            "Processed 2000 books.\n",
            "Processed 3000 books.\n",
            "Processed 4000 books.\n",
            "Processed 5000 books.\n",
            "Processed 6000 books.\n",
            "Processed 7000 books.\n",
            "Processed 8000 books.\n",
            "Processed 9000 books.\n",
            "Processed 10000 books.\n",
            "Processed 11000 books.\n",
            "Processed 12000 books.\n",
            "Processed 13000 books.\n",
            "Processed 14000 books.\n",
            "Processed 15000 books.\n",
            "Processed 16000 books.\n",
            "Processed 17000 books.\n",
            "Processed 18000 books.\n",
            "Processed 19000 books.\n",
            "Processed 20000 books.\n",
            "Processed 21000 books.\n",
            "Processed 22000 books.\n",
            "Processed 23000 books.\n",
            "Processed 24000 books.\n",
            "Processed 25000 books.\n",
            "Processed 26000 books.\n",
            "Processed 27000 books.\n",
            "Processed 28000 books.\n",
            "Processed 29000 books.\n",
            "Processed 30000 books.\n",
            "Processed 31000 books.\n",
            "Processed 32000 books.\n",
            "Processed 33000 books.\n",
            "Processed 34000 books.\n",
            "Processed 35000 books.\n",
            "Processed 36000 books.\n",
            "Processed 37000 books.\n",
            "Processed 38000 books.\n",
            "Processed 39000 books.\n",
            "Processed 40000 books.\n",
            "Processed 41000 books.\n",
            "Processed 42000 books.\n",
            "Processed 43000 books.\n",
            "Processed 44000 books.\n",
            "Processed 45000 books.\n",
            "Processed 46000 books.\n",
            "Processed 47000 books.\n",
            "Processed 48000 books.\n",
            "Processed 49000 books.\n",
            "Processed 50000 books.\n",
            "Processed 51000 books.\n",
            "Processed 52000 books.\n",
            "Processed 53000 books.\n",
            "Processed 54000 books.\n",
            "Processed 55000 books.\n",
            "Processed 56000 books.\n",
            "Processed 57000 books.\n",
            "Processed 58000 books.\n",
            "Processed 59000 books.\n",
            "Processed 60000 books.\n",
            "Processed 61000 books.\n",
            "Processed 62000 books.\n",
            "Processed 63000 books.\n",
            "Processed 64000 books.\n",
            "Processed 65000 books.\n",
            "Processed 66000 books.\n",
            "Processed 67000 books.\n",
            "Processed 68000 books.\n",
            "Processed 69000 books.\n",
            "Processed 70000 books.\n",
            "Processed 71000 books.\n",
            "Processed 72000 books.\n",
            "Processed 73000 books.\n",
            "Processed 74000 books.\n",
            "Processed 75000 books.\n",
            "Processed 76000 books.\n",
            "Processed 77000 books.\n",
            "Processed 78000 books.\n",
            "Processed 79000 books.\n",
            "Processed 80000 books.\n",
            "Processed 81000 books.\n",
            "Processed 82000 books.\n",
            "Processed 83000 books.\n",
            "Processed 84000 books.\n",
            "Processed 85000 books.\n",
            "Processed 86000 books.\n",
            "Processed 87000 books.\n",
            "Processed 88000 books.\n",
            "Processed 89000 books.\n",
            "Processed 90000 books.\n",
            "Processed 91000 books.\n",
            "Processed 92000 books.\n",
            "Processed 93000 books.\n",
            "Processed 94000 books.\n",
            "Processed 95000 books.\n",
            "Processed 96000 books.\n",
            "Processed 97000 books.\n",
            "Processed 98000 books.\n",
            "Processed 99000 books.\n",
            "Processed 100000 books.\n",
            "Processed 101000 books.\n",
            "Processed 102000 books.\n",
            "Processed 103000 books.\n",
            "Processed 104000 books.\n",
            "Processed 105000 books.\n",
            "Processed 106000 books.\n",
            "Processed 107000 books.\n",
            "Processed 108000 books.\n",
            "Processed 109000 books.\n",
            "Processed 110000 books.\n",
            "Processed 111000 books.\n",
            "Processed 112000 books.\n",
            "Processed 113000 books.\n",
            "Processed 114000 books.\n",
            "Processed 115000 books.\n",
            "Processed 116000 books.\n",
            "Processed 117000 books.\n",
            "Processed 118000 books.\n",
            "Processed 119000 books.\n",
            "Processed 120000 books.\n",
            "Processed 121000 books.\n",
            "Processed 122000 books.\n",
            "Processed 123000 books.\n",
            "Processed 124000 books.\n",
            "Processed 124082 books.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e3c5de48-8477-4ba1-9b68-1e58d1a3c39c\", \"raw_books_chunk.parquet\", 221921719)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(book_data))\n",
        "print(len(book_data[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPlT_80vI5lp",
        "outputId": "aa7efae3-68cb-409f-bc38-6c358013abac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124082\n",
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There should be 124082 books and 29 attributes."
      ],
      "metadata": {
        "id": "q9n4WY4pTJfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean data"
      ],
      "metadata": {
        "id": "C8GT6RWBVDwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove unused columns\n",
        "columns_to_keep = [\n",
        "    'text_reviews_count', 'country_code', 'average_rating', 'num_pages',\n",
        "    'ratings_count', 'title', 'publication_year', 'publication_month', 'publication_day'\n",
        "]\n",
        "\n",
        "def is_valid_date(date_string, date_format=\"%Y-%m-%d\"):\n",
        "    try:\n",
        "        datetime.strptime(date_string, date_format)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def clean_book_data(books_data, columns_to_keep):\n",
        "    cleaned_books = []\n",
        "    required_columns = set(columns_to_keep)\n",
        "    seen_titles_ratings = set()\n",
        "    TODAY_YEAR = 2024\n",
        "    TODAY_MONTH = 12\n",
        "    TODAY_DAY = 1\n",
        "    clean = True\n",
        "\n",
        "    for book in books_data:\n",
        "        clean = True\n",
        "\n",
        "        # Check if all required columns exist\n",
        "        if not required_columns.issubset(book.keys()):\n",
        "            continue  # Skip the book if required columns are missing\n",
        "\n",
        "        # Check if any required column has an empty string value\n",
        "        if any(book[column] == '' for column in required_columns):\n",
        "            continue  # Skip the book if any column has an empty string\n",
        "\n",
        "        # Filter only the relevant columns and remove entries with empty string values\n",
        "        cleaned_book = {key: book[key] for key in required_columns if key in book and book[key] != ''}\n",
        "\n",
        "        # Clean other fields\n",
        "        cleaned_book[\"text_reviews_count\"] = int(cleaned_book.get(\"text_reviews_count\", \"0\"))\n",
        "        cleaned_book[\"average_rating\"] = float(cleaned_book.get(\"average_rating\", \"0.00\"))\n",
        "        cleaned_book[\"num_pages\"] = int(cleaned_book.get(\"num_pages\", \"0\"))\n",
        "        cleaned_book[\"ratings_count\"] = int(cleaned_book.get(\"ratings_count\", \"0\"))\n",
        "        cleaned_book[\"publication_year\"] = int(cleaned_book.get(\"publication_year\", \"0\"))\n",
        "        cleaned_book[\"publication_month\"] = int(cleaned_book.get(\"publication_month\", \"0\"))\n",
        "        cleaned_book[\"publication_day\"] = int(cleaned_book.get(\"publication_day\", \"0\"))\n",
        "\n",
        "        # Check if the (title, average_rating) pair has been seen already (i.e., check for duplicates)\n",
        "        title_rating_pair = (cleaned_book[\"title\"], cleaned_book[\"average_rating\"])\n",
        "        if title_rating_pair not in seen_titles_ratings and float(title_rating_pair[1]) >= 1:\n",
        "            seen_titles_ratings.add(title_rating_pair)\n",
        "        else:\n",
        "          clean = False\n",
        "\n",
        "        if cleaned_book[\"publication_year\"] > TODAY_YEAR:\n",
        "          clean = False\n",
        "        elif cleaned_book[\"publication_year\"] == TODAY_YEAR and cleaned_book[\"publication_month\"] > TODAY_MONTH:\n",
        "          clean = False\n",
        "        elif cleaned_book[\"publication_year\"] == TODAY_YEAR and cleaned_book[\"publication_month\"] == TODAY_MONTH and cleaned_book[\"publication_day\"] > TODAY_DAY:\n",
        "          clean = False\n",
        "\n",
        "        if cleaned_book[\"ratings_count\"] == 0:\n",
        "          clean = False\n",
        "\n",
        "        if cleaned_book[\"ratings_count\"] < cleaned_book[\"text_reviews_count\"]:\n",
        "          clean = False\n",
        "\n",
        "        date_str = str(cleaned_book[\"publication_year\"]) + \"-\" + str(cleaned_book[\"publication_month\"]) + \"-\" + str(cleaned_book[\"publication_day\"])\n",
        "        if not is_valid_date(date_str):\n",
        "          clean = False\n",
        "\n",
        "        if clean:\n",
        "          # Add the cleaned book if it passes condictions\n",
        "          cleaned_books.append(cleaned_book)\n",
        "\n",
        "    return cleaned_books\n",
        "\n",
        "cleaned_book_data = clean_book_data(book_data, columns_to_keep)"
      ],
      "metadata": {
        "id": "jkAF7sFGVG6r"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(cleaned_book_data))\n",
        "print(cleaned_book_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68MChpyBdQQk",
        "outputId": "56bfec4b-573e-4f30-a01e-9db83e7997ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65118\n",
            "{'country_code': 'US', 'title': 'The Aeneid for Boys and Girls', 'num_pages': 162, 'publication_year': 2006, 'average_rating': 4.13, 'text_reviews_count': 7, 'publication_month': 9, 'ratings_count': 46, 'publication_day': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Cleaned data to local device\n",
        "Code below saves data to this device."
      ],
      "metadata": {
        "id": "axwY0s8iaBUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save cleaned data\n",
        "def process_in_chunks(book_data, chunk_size=1000, output_file=\"./genre/cleaned_books_chunk.parquet\"):\n",
        "    \"\"\"Process the data in chunks to reduce memory consumption and append to Parquet.\"\"\"\n",
        "    chunk = []\n",
        "\n",
        "    #remove file if exists\n",
        "    if os.path.exists(output_file):\n",
        "      os.remove(output_file)\n",
        "\n",
        "    # Start by writing the first chunk to the Parquet file\n",
        "    for i, book in enumerate(book_data):\n",
        "        chunk.append(book_data[i])\n",
        "\n",
        "        # When chunk size is reached or it's the last chunk, process and clear memory\n",
        "        if (i + 1) % chunk_size == 0 or i + 1 == len(book_data):\n",
        "            # Convert to DataFrame\n",
        "            df_chunk = pd.DataFrame(chunk)\n",
        "\n",
        "            # Check if the file already exists\n",
        "            try:\n",
        "                # Try reading the existing Parquet file to append\n",
        "                existing_df = pd.read_parquet(output_file)\n",
        "                # Concatenate the new chunk to the existing data\n",
        "                df_chunk = pd.concat([existing_df, df_chunk], ignore_index=True)\n",
        "            except FileNotFoundError:\n",
        "                # If the file doesn't exist, this is the first chunk to write\n",
        "                pass\n",
        "\n",
        "            # Write the concatenated DataFrame to Parquet (overwrites the file if it exists)\n",
        "            df_chunk.to_parquet(output_file, index=False)\n",
        "\n",
        "            # Clear the chunk list to free memory\n",
        "            chunk = []\n",
        "            print(f\"Processed {i + 1} books.\")\n",
        "\n",
        "cleaned_parquet_file = \"./genre/cleaned_books.parquet\"\n",
        "process_in_chunks(cleaned_book_data, output_file=cleaned_parquet_file)\n",
        "files.download(cleaned_parquet_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PmgRTxX4Wjxj",
        "outputId": "75398123-d025-402e-91af-e616efad5526"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1000 books.\n",
            "Processed 2000 books.\n",
            "Processed 3000 books.\n",
            "Processed 4000 books.\n",
            "Processed 5000 books.\n",
            "Processed 6000 books.\n",
            "Processed 7000 books.\n",
            "Processed 8000 books.\n",
            "Processed 9000 books.\n",
            "Processed 10000 books.\n",
            "Processed 11000 books.\n",
            "Processed 12000 books.\n",
            "Processed 13000 books.\n",
            "Processed 14000 books.\n",
            "Processed 15000 books.\n",
            "Processed 16000 books.\n",
            "Processed 17000 books.\n",
            "Processed 18000 books.\n",
            "Processed 19000 books.\n",
            "Processed 20000 books.\n",
            "Processed 21000 books.\n",
            "Processed 22000 books.\n",
            "Processed 23000 books.\n",
            "Processed 24000 books.\n",
            "Processed 25000 books.\n",
            "Processed 26000 books.\n",
            "Processed 27000 books.\n",
            "Processed 28000 books.\n",
            "Processed 29000 books.\n",
            "Processed 30000 books.\n",
            "Processed 31000 books.\n",
            "Processed 32000 books.\n",
            "Processed 33000 books.\n",
            "Processed 34000 books.\n",
            "Processed 35000 books.\n",
            "Processed 36000 books.\n",
            "Processed 37000 books.\n",
            "Processed 38000 books.\n",
            "Processed 39000 books.\n",
            "Processed 40000 books.\n",
            "Processed 41000 books.\n",
            "Processed 42000 books.\n",
            "Processed 43000 books.\n",
            "Processed 44000 books.\n",
            "Processed 45000 books.\n",
            "Processed 46000 books.\n",
            "Processed 47000 books.\n",
            "Processed 48000 books.\n",
            "Processed 49000 books.\n",
            "Processed 50000 books.\n",
            "Processed 51000 books.\n",
            "Processed 52000 books.\n",
            "Processed 53000 books.\n",
            "Processed 54000 books.\n",
            "Processed 55000 books.\n",
            "Processed 56000 books.\n",
            "Processed 57000 books.\n",
            "Processed 58000 books.\n",
            "Processed 59000 books.\n",
            "Processed 60000 books.\n",
            "Processed 61000 books.\n",
            "Processed 62000 books.\n",
            "Processed 63000 books.\n",
            "Processed 64000 books.\n",
            "Processed 65000 books.\n",
            "Processed 65118 books.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f9ad3bc0-0353-466b-a868-f7e264784ade\", \"cleaned_books.parquet\", 2108300)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Data\n",
        "This section tests if the data are cleaned and ready for use"
      ],
      "metadata": {
        "id": "_6SKS-yBYolS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_cleaned_books_data(books_data, columns_to_keep):\n",
        "    # Check that each book contains all required columns\n",
        "    required_columns = set(columns_to_keep)\n",
        "\n",
        "    for idx, book in enumerate(books_data):\n",
        "        # Check if all required columns exist\n",
        "        if not required_columns.issubset(book.keys()):\n",
        "            print(f\"Book {idx} is missing required columns: {required_columns - set(book.keys())}\")\n",
        "            return False\n",
        "\n",
        "        # Check if any column has an empty string value\n",
        "        for column in required_columns:\n",
        "            if book[column] == '':\n",
        "                print(f\"Book {idx} has an empty string in column '{column}'\")\n",
        "                return False\n",
        "\n",
        "    print(\"Data is clean and ready for use!\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# Columns to check against\n",
        "columns_to_keep = [\n",
        "    'text_reviews_count', 'country_code', 'average_rating', 'num_pages',\n",
        "    'ratings_count', 'title', 'publication_year', 'publication_month', 'publication_day'\n",
        "]\n",
        "\n",
        "# Run the test\n",
        "is_data_clean = test_cleaned_books_data(cleaned_book_data, columns_to_keep)\n",
        "\n",
        "# Print the result\n",
        "print(\"Is the data clean:\", is_data_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me4djmmdYn2d",
        "outputId": "d313f6d6-b476-46a3-cf3f-f8f5ee0759a3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is clean and ready for use!\n",
            "Is the data clean: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(cleaned_book_data))\n",
        "print(len(cleaned_book_data[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8dBBLkaZF8S",
        "outputId": "8ecdeb75-077c-46d3-8615-aedc17615de5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65118\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 76600 books after processing."
      ],
      "metadata": {
        "id": "FGzVFWm_ZELZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "This section bulids a model that predicts average rating."
      ],
      "metadata": {
        "id": "J7d7ZQeddCkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model(Bert)"
      ],
      "metadata": {
        "id": "XLPO6SlxODhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Hyperparameters\n",
        "criterion = nn.MSELoss()\n",
        "MAX_LEN = 30\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 0.000011\n",
        "FROZEN = False\n",
        "SEED = 304\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "\n",
        "# Dataset Class\n",
        "class BookDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        title = item['title']\n",
        "        rating = float(item['average_rating'])\n",
        "        encoding = self.tokenizer(\n",
        "            title,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].squeeze(0)\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'rating': torch.tensor(rating, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "class BertForRegression(nn.Module):\n",
        "    def __init__(self, pretrained_model_name='bert-base-uncased'):\n",
        "        super(BertForRegression, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
        "        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)  # Single regression output\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.pooler_output  # Using the pooler output (embedding of the [CLS] token)\n",
        "        regression_output = self.regressor(cls_output)\n",
        "        return regression_output\n",
        "\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForRegression(pretrained_model_name='bert-base-uncased').to(device)\n",
        "\n",
        "# Prepare Dataset and DataLoader\n",
        "dataset = BookDataset(cleaned_book_data, tokenizer, MAX_LEN)\n",
        "\n",
        "# Split into train, validation, and test datasets\n",
        "train_data, val_test_data = train_test_split(dataset.data, test_size=0.3, random_state=SEED)\n",
        "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=SEED)\n",
        "\n",
        "# Create datasets and dataloaders for train, validation, and test sets\n",
        "train_dataset = BookDataset(train_data, tokenizer, MAX_LEN)\n",
        "val_dataset = BookDataset(val_data, tokenizer, MAX_LEN)\n",
        "test_dataset = BookDataset(test_data, tokenizer, MAX_LEN)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "if FROZEN:\n",
        "  for name, param in model.named_parameters():\n",
        "    # Only compute gradients for parameters of our\n",
        "    # newly added regressor. BERT will not be trained.\n",
        "    if 'regressor' not in name:\n",
        "      param.requires_grad = False\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
        "        tepoch.set_description(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        for batch in tepoch:\n",
        "            # Move input tensors to the correct device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            ratings = batch['rating'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = outputs.squeeze(-1)  # Flatten the output to match ratings shape\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = F.mse_loss(predictions, ratings)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Update progress bar description with the current loss\n",
        "            tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Validation Loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        with tqdm(val_dataloader, unit=\"batch\") as vepoch:\n",
        "            vepoch.set_description(f\"Validation {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "            for batch in vepoch:\n",
        "                # Move input tensors to the correct device\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                ratings = batch['rating'].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                predictions = outputs.squeeze(-1)  # Flatten the output to match ratings shape\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = F.mse_loss(predictions, ratings)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "                # Update progress bar description with the current loss\n",
        "                vepoch.set_postfix(val_loss=loss.item())\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekmdm9dqWB8U",
        "outputId": "bedd1a1b-cf99-47a5-9710-ffcbd1bcd6b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 100%|██████████| 2849/2849 [05:21<00:00,  8.87batch/s, loss=0.22]\n",
            "Validation 1/3: 100%|██████████| 611/611 [00:20<00:00, 29.67batch/s, val_loss=0.214]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Train Loss: 0.2077, Validation Loss: 0.1325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3:  16%|█▌        | 442/2849 [00:49<04:40,  8.57batch/s, loss=0.0931]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Evaluation on Test Data with Accuracy and R² Calculation\n",
        "model.eval()\n",
        "total_test_loss = 0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "accuracy_margin = 0.25\n",
        "true_ratings = []\n",
        "predicted_ratings = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
        "        tepoch.set_description(\"Evaluating Test Set\")\n",
        "        for batch in tepoch:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            ratings = batch['rating'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(input_ids=input_ids, attention_mask=attention_mask).squeeze(-1)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(predictions, ratings)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy: check if prediction is within 0.25 of the actual rating\n",
        "            correct_predictions_batch = torch.abs(predictions - ratings) <= accuracy_margin\n",
        "            correct_predictions += correct_predictions_batch.sum().item()\n",
        "            total_predictions += len(ratings)\n",
        "\n",
        "            # Store true and predicted ratings for R² computation\n",
        "            true_ratings.extend(ratings.cpu().numpy())\n",
        "            predicted_ratings.extend(predictions.cpu().numpy())\n",
        "\n",
        "            tepoch.set_postfix(test_loss=loss.item(), accuracy=(correct_predictions / total_predictions) * 100)\n",
        "\n",
        "# Compute final test accuracy\n",
        "accuracy = (correct_predictions / total_predictions) * 100\n",
        "avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "\n",
        "# Compute R² score\n",
        "r2 = r2_score(true_ratings, predicted_ratings)\n",
        "\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "print(f\"Test Accuracy (within 0.25 of target): {accuracy:.2f}%\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0MvR2EnZXA9",
        "outputId": "372d6f51-fb83-466a-e0c3-3ce884767a6e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Test Set: 100%|██████████| 611/611 [00:20<00:00, 29.67batch/s, accuracy=58, test_loss=0.0602]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1271\n",
            "Test Accuracy (within 0.25 of target): 57.95%\n",
            "R² Score: 0.0794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model's state_dict to a file in Colab's local environment\n",
        "torch.save(model.state_dict(), '/content/book_rating.pth')\n",
        "\n",
        "# Optionally, save the optimizer's state as well\n",
        "torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss,\n",
        "}, '/content/checkpoint.pth')\n",
        "\n",
        "print(\"Model saved to /content/\")\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Download the model\n",
        "files.download('/content/book_rating.pth')"
      ],
      "metadata": {
        "id": "hCU34B10iMzT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1c7be332-2f35-4e16-c7dc-df9f281237fb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f9383a22-4954-4c35-b0bf-3e6a519048ed\", \"book_rating.pth\", 438014484)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example For a Tokenized Input"
      ],
      "metadata": {
        "id": "b-mFBL2MuBtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example For token\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "df = pd.DataFrame(cleaned_book_data)\n",
        "\n",
        "inputs = df.title.values\n",
        "\n",
        "tokenized_inputs = bert_tokenizer(\n",
        "    inputs.tolist(),          # Input text\n",
        "    add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n",
        "    padding='max_length',\n",
        "    truncation=True, # pad to a length specified by the max_length\n",
        "    max_length=MAX_LEN,       # truncate all sentences longer than max_length\n",
        "    return_tensors='pt',      # return everything we need as PyTorch tensors\n",
        ")\n",
        "\n",
        "input_ids = tokenized_inputs['input_ids']\n",
        "attention_masks = tokenized_inputs['attention_mask']"
      ],
      "metadata": {
        "id": "A5GJLQ_ZWEez"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "req = 23\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', tokenized_inputs['input_ids'][req])\n",
        "print('* Token IDs:', tokenized_inputs['attention_mask'][req])\n",
        "print('* Tokenized:', bert_tokenizer.decode(tokenized_inputs['input_ids'][req]))\n",
        "print('* Attention_mask', tokenized_inputs['attention_mask'][req])\n",
        "\n",
        "# Get the token IDs and decode them back to tokens\n",
        "input_ids = tokenized_inputs['input_ids'][req].squeeze(0)  # Get the tensor and remove batch dimension\n",
        "tokens = bert_tokenizer.convert_ids_to_tokens(input_ids)  # Convert IDs back to tokens\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN_cMxt9i-C8",
        "outputId": "c6259b52-13f9-4567-adb6-d631db633802"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  tensor([  101,  9999,  2395, 24115,  1024,  3500,  7292, 28766,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "* Token IDs: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0])\n",
            "* Tokenized: [CLS] bradford street buddies : springtime blossoms [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "* Attention_mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0])\n",
            "['[CLS]', 'bradford', 'street', 'buddies', ':', 'spring', '##time', 'blossoms', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "v2-rSgRybkjc",
        "y1fjiBiQZ0AX",
        "axwY0s8iaBUt"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}